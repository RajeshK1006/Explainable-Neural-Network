## Abstract

This project investigates the explainability and interpretability of neural networks by utilizing advanced Explainable AI (XAI) techniques, specifically focusing on saliency maps and activation function visualizations. We employ a fully connected neural network model trained on the MNIST dataset to enhance our understanding of the processes involved in classifying handwritten digits. 

By implementing saliency maps, we visualize pixel-level influences that significantly contribute to the model's predictions, providing valuable insights into feature importance. Furthermore, we analyze activations from various layers of the network to explore the learned representations at different processing stages.

Throughout the training and evaluation phases, we present insights into key performance metrics, including cross-entropy loss and accuracy, while leveraging visual tools to promote transparency in the decision-making processes of neural networks. The outcomes of this study aim to improve model interpretability and foster user trust in AI systems by clarifying the underlying mechanisms of deep learning models.
